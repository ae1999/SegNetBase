{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "increased-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tqdm\n",
    "import itertools\n",
    "import numpy as np\n",
    "from tqdm.contrib import tzip\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import convolve\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "precious-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "willing-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_reader():\n",
    "    def __init__(self, DATA_PATH = '../../data/part_A/camvid/', labels = False):\n",
    "        \n",
    "        self.DATA_PATH = DATA_PATH\n",
    "        \n",
    "        self.label_map = {}\n",
    "        self.color_map = []\n",
    "        self.reverse_translate = {}\n",
    "\n",
    "        txt = open(DATA_PATH + 'label_colors.txt').readlines()\n",
    "        for line in txt:\n",
    "            self.label_map[line[:-1].split('\\t')[-1]] = line.split('\\t')[0].split(' ')\n",
    "            self.color_map.append(line.split('\\t')[0].replace(\" \", \"\"))\n",
    "        l = 0\n",
    "        for i, j in self.label_map.items():\n",
    "            self.reverse_translate[l] = [int(k) for k in j]\n",
    "            l += 1\n",
    "        if labels:\n",
    "            print('Number of colors:', len(self.color_map))\n",
    "            for i, j in self.label_map.items():\n",
    "                print(i, j)\n",
    "            for i, j in self.reverse_translate.items():\n",
    "                print(i, j)\n",
    "                \n",
    "                \n",
    "        self.labels_count = [0 for i in range(len(self.color_map))]\n",
    "        \n",
    "    def rev_translate(self, res):\n",
    "        mask = []\n",
    "        for i in range(res.shape[1]):\n",
    "            mask_temp = []\n",
    "            for j in range(res.shape[2]):\n",
    "                mask_temp.append(self.reverse_translate[int(res[0][i][j])])\n",
    "            mask.append(mask_temp)\n",
    "        return mask\n",
    "    \n",
    "    def color_translate(self, color):\n",
    "        if len(color) > 3:\n",
    "            print('err: error in color_translate')\n",
    "        code = str(color[2]) + str(color[1]) + str(color[0])\n",
    "        color_code = self.color_map.index(code)\n",
    "        if not code in self.color_map:\n",
    "            print('err: label color is not defined', code)\n",
    "            color_code = self.color_map.index('000')\n",
    "        if color_code > 31 or color_code < 0:\n",
    "            print('err: label color is not defined', code)\n",
    "        self.labels_count[self.color_map.index(code)] += 1\n",
    "        return color_code\n",
    "\n",
    "    def one_hot(self, pic):\n",
    "        label = np.zeros([210, 280])\n",
    "        for i in range(210):\n",
    "            for j in range(280):\n",
    "                label[i, j] = self.color_translate(pic[i][j])\n",
    "        return label\n",
    "    \n",
    "    def labels_probability(self):\n",
    "        values = np.array(self.labels_count)\n",
    "        p_values = values/np.sum(values)\n",
    "        return torch.Tensor(p_values)\n",
    "    \n",
    "    def load_data(self, split = True, data_shape_log = False):\n",
    "        \n",
    "        DATA_PATH = self.DATA_PATH\n",
    "        data = []\n",
    "        labels = []\n",
    "\n",
    "        ims = [DATA_PATH + 'images/' + i for i in os.listdir(DATA_PATH + 'images/')]\n",
    "        lbs = [DATA_PATH + 'labels/' + i[:-4] + '_L' + i[-4:] for i in os.listdir(DATA_PATH + 'images/')]\n",
    "\n",
    "        if(len(ims) != len(lbs)):\n",
    "            print('err: size of images and labels are not the same.')\n",
    "\n",
    "        print('Images size:', cv2.imread(ims[0]).shape)\n",
    "\n",
    "        plt.imshow(np.array(cv2.imread(ims[0])))\n",
    "        plt.show()\n",
    "        plt.imshow(np.array(cv2.imread(lbs[0])))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        for i, j in tzip(ims, lbs):\n",
    "            try:\n",
    "                datum = cv2.resize(cv2.imread(i), (280, 210))\n",
    "                label = self.one_hot(cv2.resize(cv2.imread(j), (280, 210), interpolation=cv2.INTER_NEAREST))\n",
    "                data.append(datum)\n",
    "                labels.append(label)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if not split:\n",
    "            return data, labels\n",
    "        \n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.42, random_state=42)\n",
    "        \n",
    "        if data_shape_log:\n",
    "            print('data size:\\t', len(data), len(labels))\n",
    "            print('X_train:\\ttraining images shape\\t', len(X_train), X_train[0].shape)\n",
    "            print('X_test:\\t\\ttest images shape\\t', len(X_test), X_test[0].shape)\n",
    "            print('Y_train:\\ttraining labels shape\\t', len(Y_train), Y_train[0].shape)\n",
    "            print('Y_test:\\t\\ttest labels shape\\t', len(Y_test), Y_test[0].shape)\n",
    "        \n",
    "        return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaptive-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CamvidDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, transform):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, index):\n",
    "        image = self.X[index]\n",
    "        label = self.y[index]\n",
    "        image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "regulation-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img, t = ''):\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(t)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_x86",
   "language": "python",
   "name": "pytorch_x86"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
